{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import runpy\n",
    "import scipy\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySQL connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mydb = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"pathofpain\"\n",
    "    )\n",
    "    print(\"Connection established\")\n",
    "    cursor = mydb.cursor(buffered=True)\n",
    "    cursor.execute(\"use `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(\"An error occurred:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySQL DATABASE \"SERS_ML_TEST\" create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "\n",
    "# Open the .sql file\n",
    "sql_file = open('SERS-ML_test_createDB.sql','r')\n",
    "\n",
    "# Read the .sql queries\n",
    "sql_command = sql_file.read()\n",
    "\n",
    "# Execute Multi statements in .sql script\n",
    "for result in cursor.execute(sql_command, multi=True):\n",
    "    print(result)\n",
    "\n",
    "# Commit Change to database\n",
    "mydb.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os. getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please enter the directory\n",
    "data_dir = 'Exp_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirpath, dirnames, filenames in os.walk('Exp_Data'):\n",
    "    print(dirpath)\n",
    "    print(dirnames)\n",
    "    print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirpath, dirnames, filenames in os.walk('Exp_Data/'):\n",
    "    print(dirpath)\n",
    "    print(dirnames)\n",
    "    print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_ba_path = 'SERS_debaseline.py'\n",
    "\n",
    "k = 0\n",
    "for dirpath, dirnames, filenames in os.walk(data_dir):\n",
    "    for filenames_index in filenames :\n",
    "        print(filenames_index)\n",
    "        folder_name = filenames_index.split('.')[0]\n",
    "        label = folder_name\n",
    "        # folder_name = folder_name + '_' + str(k)  \n",
    "        print(folder_name)\n",
    "        print(filenames_index)\n",
    "        dir = dirpath + '/' + folder_name +'/'\n",
    "        print(dir)\n",
    "        os.makedirs(dir, exist_ok = True)\n",
    "\n",
    "        file = dirpath + '/' + filenames_index\n",
    "        dest = dir + '/' + label + '.txt'\n",
    "        de_path = dir+'/' + 'debase.py'\n",
    "        shutil.move(file, dest)\n",
    "        shutil.copy(de_ba_path, de_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for dirpath, dirnames, filenames in os.walk(data_dir):\n",
    "    for filenames_index in filenames :\n",
    "        if filenames_index == 'debase.py':\n",
    "            fi = dirpath + '/' + filenames_index\n",
    "            file_globals = runpy.run_path(fi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAW data record into database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "k = 0\n",
    "for dirpath, dirnames, filenames in os.walk(data_dir):\n",
    "    if dirpath[-2:] == 'sb':\n",
    "        \n",
    "        df = pd.read_csv(dirpath + '/'+ filenames[0])\n",
    "        count = df.shape[0]\n",
    "        print(count)\n",
    "\n",
    "        print(filenames[0])\n",
    "        label = '_'.join(filenames[0].split(\"_\")[1:-3])\n",
    "        print(label)\n",
    "\n",
    "        info = label.split(\"_\")\n",
    "        print(info)\n",
    "\n",
    "        bacteria = info[0].split(\"-\")[0]\n",
    "        print (bacteria)\n",
    "\n",
    "        strain = info[0].split(\"-\")[1]\n",
    "        print(strain)\n",
    "\n",
    "        subtype = 'WT'\n",
    "        if (len(info[0].split(\"-\")) == 3):\n",
    "            subtype = info[0].split(\"-\")[2]\n",
    "        print(subtype)\n",
    "\n",
    "\n",
    "        antibiotic = info[1][0:3] \n",
    "        print(antibiotic)\n",
    "\n",
    "        anti_conc = info[1][3:] \n",
    "        print(anti_conc)\n",
    "        \n",
    "        anti_time =  info[2]\n",
    "        print(anti_time)\n",
    "\n",
    "        sql_command = \"\"\"INSERT INTO `Lab_Groups`(`file_name`,`bacteria`,`strain`,`subtype`,`antibiotic`, `anti_conc`,`anti_time`, `data_path`,`count`)\n",
    "               VALUES ('{file_name}','{bacteria}','{strain}','{subtype}','{antibiotic}', '{anti_conc}','{anti_time}', '{data_path}','{count}');\"\"\".format(\n",
    "                file_name = filenames[0], bacteria = bacteria, strain=strain, subtype=subtype, antibiotic=antibiotic, anti_conc=anti_conc,\n",
    "                            anti_time = anti_time, data_path = dirpath + '/' + filenames[0], count = count\n",
    "               )\n",
    "\n",
    "\n",
    "        print (sql_command)\n",
    "        cursor.execute(sql_command)\n",
    "        mydb.commit()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "antibiotic = 'Amp'\n",
    "anti_conc = 16\n",
    "anti_time = '30min'\n",
    "\n",
    "\n",
    "command = (\"\"\"SELECT `data_path`, `file_name`, `id` FROM `Lab_Groups`\n",
    "            WHERE (`antibiotic` = '{antibiotic}') AND (`anti_conc` = '{anti_conc}') AND (`anti_time` = '{anti_time}');\"\"\".format(\n",
    "            antibiotic = antibiotic, anti_conc = anti_conc, anti_time = anti_time\n",
    "))\n",
    "\n",
    "cursor.execute(command)\n",
    "groups_list = cursor.fetchall()\n",
    "print(groups_list)\n",
    "\n",
    "wave = np.arange(400, 1550.1, 0.5, dtype=float)\n",
    "feature = np.zeros((1,2301))\n",
    "labels = []\n",
    "lab_groups = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for group_file, filename, group_id in groups_list:\n",
    "    print(group_file)\n",
    "    print(filename)\n",
    "    print(group_id)\n",
    "        \n",
    "    df = pd.read_csv(group_file)\n",
    "    df = df.loc[:, '400':'1550']\n",
    "    df = df.to_numpy()\n",
    "    feature = np.row_stack((feature, df))\n",
    "    label = '_'.join(filename.split(\"_\")[1:-3])\n",
    "    print(label)\n",
    "    labels.extend([label]*df.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature = feature[1:,:]\n",
    "print(feature.shape)\n",
    "df = pd.DataFrame(feature, columns=wave)\n",
    "df.insert(0, 'label', labels)\n",
    "count = df.shape[0]\n",
    "df.to_csv(data_dir + 'RAW_SERS_dataset.csv', index= False)\n",
    "\n",
    "\n",
    "## Record RAW dataset into Database\n",
    "sql_command = \"\"\"INSERT INTO `Combine_SERS`(`file_name`,`antibiotic`, `anti_conc`,`anti_time`, `data_path`,`count`)\n",
    "               VALUES ('{file_name}','{antibiotic}', '{anti_conc}','{anti_time}', '{data_path}','{count}');\"\"\".format(\n",
    "                file_name = 'RAW_SERS_dataset.csv', antibiotic=antibiotic, anti_conc=anti_conc, anti_time = anti_time, \n",
    "                data_path = data_dir + 'RAW_SERS_dataset.csv', count = count)\n",
    "\n",
    "print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n",
    "\n",
    "\n",
    "## Record relationship of lab_groups and combine_dataset in database\n",
    "cursor.execute(\"\"\"SELECT `id` FROM `Combine_SERS`\n",
    "        WHERE (`antibiotic` = '{antibiotic}') AND (`anti_conc` = '{anti_conc}') AND (`anti_time` = '{anti_time}');\"\"\".format(\n",
    "        antibiotic = antibiotic, anti_conc = anti_conc, anti_time = anti_time))\n",
    "dataset_id = cursor.fetchall()[0][0]\n",
    "\n",
    "for group_file, filename, group_id in groups_list:\n",
    "    print(group_id)\n",
    "    cursor.execute(\"\"\"INSERT INTO `Combine_Groups`(`combine_id`, `lab_group_id`)\n",
    "                   VALUES('{dataset_id}', '{group_id}');\"\"\".format(\n",
    "                    dataset_id = dataset_id, group_id = group_id))\n",
    "    mydb.commit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning by 733/690 ratio >= 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning by 733/690 ratio >= 1.2\n",
    "\n",
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "# experimental conditions\n",
    "antibiotic = 'Amp'\n",
    "anti_conc = 16\n",
    "anti_time = '30min'\n",
    "\n",
    "\n",
    "cursor.execute(\"\"\"SELECT `data_path`,`id` FROM `Combine_SERS`\n",
    "        WHERE (`antibiotic` = '{antibiotic}') AND (`anti_conc` = '{anti_conc}') AND (`anti_time` = '{anti_time}');\"\"\".format(\n",
    "        antibiotic = antibiotic, anti_conc = anti_conc, anti_time = anti_time))\n",
    "RAW_dataset = cursor.fetchall()\n",
    "RAW_file = RAW_dataset[0][0]\n",
    "RAW_id  = RAW_dataset[0][1]\n",
    "\n",
    "\n",
    "# df = pd.read_csv(data_dir + 'RAW_SERS_dataset.csv', header = 0)\n",
    "df = pd.read_csv(RAW_file, header = 0)\n",
    "\n",
    "## Cleaning \n",
    "substrate_area = df.loc[:, '680.0':'700.0']\n",
    "characteristic_area = df.loc[:, '720.0':'760.0']\n",
    "substrate_peak = substrate_area.max(axis=1).to_numpy()\n",
    "characteristic_peak = characteristic_area.max(axis=1).to_numpy()\n",
    "peak_ratio = characteristic_peak / substrate_peak\n",
    "peak_ratio = peak_ratio[np.logical_not(np.isnan(peak_ratio))] \n",
    "\n",
    "\n",
    "sort_ratio_index = np.argsort(peak_ratio)\n",
    "print(sort_ratio_index)\n",
    "ratio_threshold = 1.2  ## Retain data with ratio > 1.2\n",
    "print(peak_ratio.shape)\n",
    "print(peak_ratio[sort_ratio_index])\n",
    "a = 0\n",
    "for i in range(peak_ratio.shape[0]):\n",
    "    if peak_ratio[sort_ratio_index[i]] > ratio_threshold:\n",
    "        a = i\n",
    "        break\n",
    "\n",
    "\n",
    "qualified_ratio = sort_ratio_index[a:]\n",
    "qualified_ratio = list(qualified_ratio)\n",
    "qualified_ratio.sort()\n",
    "df = df.iloc[qualified_ratio, :]\n",
    "count = df.shape[0]\n",
    "df.to_csv(data_dir + 'Cleaned_SERS_dataset.csv', index= False)\n",
    "\n",
    "\n",
    "## Record Clean dataset into Database\n",
    "sql_command = \"\"\"INSERT INTO `Preprocess_SERS`\n",
    "                        (`file_name`,`antibiotic`, `anti_conc`,`anti_time`, \n",
    "                         `data_path`,`count`, `combine_id`, `cleaned`, \n",
    "                         `data_type` )\n",
    "               VALUES ('{file_name}','{antibiotic}', '{anti_conc}','{anti_time}', \n",
    "                       '{data_path}','{count}', '{combine_id}',\n",
    "                       '{cleaned}', '{data_type}');\"\"\".format(\n",
    "                file_name = 'Cleaned_SERS_dataset.csv', antibiotic=antibiotic, \n",
    "                anti_conc=anti_conc, anti_time = anti_time, \n",
    "                data_path = data_dir + 'Cleaned_SERS_dataset.csv', count = count,\n",
    "                combine_id = RAW_id, cleaned = 1, data_type = 'dataset')\n",
    "# print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistic of Cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Record Clean Statistic into database\n",
    "\n",
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "cursor.execute(\"SELECT LAST_INSERT_ID();\")\n",
    "cleaned_id = cursor.fetchall()[0][0]\n",
    "\n",
    "sql_command = \"\"\"SELECT `data_path` FROM `Preprocess_SERS`\n",
    "                WHERE `id` = {cleaned_id};\"\"\".format(cleaned_id = cleaned_id)\n",
    "cursor.execute(sql_command)\n",
    "cleaned_path = cursor.fetchall()[0][0]\n",
    "\n",
    "\n",
    "# READ Cleaned_dataset\n",
    "\n",
    "df = pd.read_csv(cleaned_path, header = 0) #data_path = data_dir + 'Cleaned_SERS_dataset.csv'\n",
    "\n",
    "\n",
    "# Counting\n",
    "clean_count = df['label'].value_counts()\n",
    "clean_count.to_csv(data_dir +'Cleaned_SERS_dataset_Count.csv', index= True)\n",
    "\n",
    "\n",
    "sql_command = \"\"\"INSERT INTO `Preprocess_SERS_Statistic`\n",
    "                        (`file_name`,`preprocess_id`, `data_type`,`data_path`)\n",
    "               VALUES ('{file_name}','{preprocess_id}', '{data_type}','{data_path}');\"\"\".format(\n",
    "                file_name = 'Cleaned_SERS_dataset_Count.csv', \n",
    "                preprocess_id = cleaned_id,\n",
    "                data_type = 'count',\n",
    "                data_path = data_dir + 'Cleaned_SERS_dataset_Count.csv')\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n",
    "## Mean of Cleaned SERS spectra\n",
    "\n",
    "df.groupby('label').mean()\n",
    "df.groupby('label').std()\n",
    "\n",
    "df_mean= df.groupby('label').mean()\n",
    "df_mean.to_csv(data_dir + 'Cleaned_SERS_dataset_mean.csv', index= True)\n",
    "\n",
    "sql_command = \"\"\"INSERT INTO `Preprocess_SERS_Statistic`\n",
    "                        (`file_name`,`preprocess_id`, `data_type`,`data_path`)\n",
    "               VALUES ('{file_name}','{preprocess_id}', '{data_type}','{data_path}');\"\"\".format(\n",
    "                file_name = 'Cleaned_SERS_dataset_mean.csv', \n",
    "                preprocess_id = cleaned_id,\n",
    "                data_type = 'mean',\n",
    "                data_path = data_dir + 'Cleaned_SERS_dataset_mean.csv')\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n",
    "\n",
    "\n",
    "## STD of Cleaned SERS spectra\n",
    "df_std= df.groupby('label').std()\n",
    "df_std.to_csv(data_dir + 'Cleaned_SERS_dataset_std.csv', index= True)\n",
    "\n",
    "sql_command = \"\"\"INSERT INTO `Preprocess_SERS_Statistic`\n",
    "                        (`file_name`,`preprocess_id`, `data_type`,`data_path`)\n",
    "               VALUES ('{file_name}','{preprocess_id}', '{data_type}','{data_path}');\"\"\".format(\n",
    "                file_name = 'Cleaned_SERS_dataset_std.csv', \n",
    "                preprocess_id = cleaned_id,\n",
    "                data_type = 'std',\n",
    "                data_path = data_dir + 'Cleaned_SERS_dataset_std.csv')\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Datalabel representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For Datalabel representation \n",
    "\n",
    "\n",
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "df = pd.read_csv(cleaned_path, header = 0)\n",
    "\n",
    "df['label'].replace({\n",
    "'Ecoli-ATCC27662_Amp16_30min': '0', \n",
    "'Ecoli-BL21_Amp16_30min': '1', \n",
    "'Ecoli-BW25113_Amp16_30min':'2', \n",
    "'Ecoli-DH5alpha_Amp16_30min':'3',   \n",
    "'Ecoli-DH5alpha-ampR_Amp16_30min':'4'}, inplace=True)\n",
    "\n",
    "count = df.shape[0]\n",
    "df.to_csv(data_dir + 'Labeled_SERS_dataset.csv', index= False)\n",
    "\n",
    "## Record Lable dataset into Database\n",
    "sql_command = \"\"\"INSERT INTO `Preprocess_SERS`\n",
    "                        (`file_name`,`antibiotic`, `anti_conc`,`anti_time`, \n",
    "                         `data_path`,`count`, `combine_id`, `data_type` ,\n",
    "                         `cleaned`, `labeled` )\n",
    "               VALUES ('{file_name}','{antibiotic}', '{anti_conc}','{anti_time}', \n",
    "                       '{data_path}','{count}', '{combine_id}','{data_type}',\n",
    "                       '{cleaned}', '{labeled}');\"\"\".format(\n",
    "                file_name = 'Labeled_SERS_dataset.csv', antibiotic = antibiotic, \n",
    "                anti_conc=anti_conc, anti_time = anti_time, \n",
    "                data_path = data_dir + 'Labeled_SERS_dataset.csv', count = count,\n",
    "                combine_id = RAW_id, data_type = 'dataset', \n",
    "                cleaned = 1, labeled = 1)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test dataset split\n",
    "\n",
    "## Directory\n",
    "data_dir = 'Exp_Data/'\n",
    "\n",
    "# experimental conditions\n",
    "antibiotic = 'Amp'\n",
    "anti_conc = 16\n",
    "anti_time = '30min'\n",
    "\n",
    "\n",
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "cursor.execute(\"\"\"SELECT `data_path`,`id` FROM `Combine_SERS`\n",
    "        WHERE (`antibiotic` = '{antibiotic}') AND (`anti_conc` = '{anti_conc}') AND (`anti_time` = '{anti_time}');\"\"\".format(\n",
    "        antibiotic = antibiotic, anti_conc = anti_conc, anti_time = anti_time))\n",
    "RAW_dataset = cursor.fetchall()\n",
    "RAW_file = RAW_dataset[0][0]\n",
    "RAW_id  = RAW_dataset[0][1]\n",
    "\n",
    "\n",
    "sql_command = \"\"\"SELECT `data_path` FROM `Preprocess_SERS`\n",
    "                WHERE `file_name` = '{file_name}';\"\"\".format(\n",
    "                    file_name = 'Labeled_SERS_dataset.csv')\n",
    "cursor.execute(sql_command)\n",
    "# cursor.fetchall()\n",
    "labeled_path = cursor.fetchall()[0][0]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(labeled_path, header = 0) #data_path = data_dir + 'Labeled_SERS_dataset.csv'\n",
    "cols = list(df.columns)\n",
    "\n",
    "# print(df['label'])\n",
    "\n",
    "combine = df.to_numpy()\n",
    "np.random.seed(400)\n",
    "np.random.shuffle(combine)\n",
    "print (combine.shape)\n",
    "\n",
    "percent10 = int (combine.shape[0] * 0.1)\n",
    "print(percent10)\n",
    "\n",
    "test_set = combine[0:percent10,:]\n",
    "train_set = combine[percent10:,:]\n",
    "\n",
    "test_set = pd.DataFrame(test_set, columns=cols)\n",
    "train_set = pd.DataFrame(train_set, columns=cols)\n",
    "\n",
    "test_count = test_set.shape[0]\n",
    "train_count = train_set.shape[0]\n",
    "\n",
    "test_set.to_csv(data_dir  + 'SERS_testing.csv', index= False , header = True)\n",
    "train_set.to_csv(data_dir + 'SERS_training.csv', index= False , header = True)\n",
    "\n",
    "## Record Test dataset into Database\n",
    "sql_command = \"\"\"INSERT INTO `Preprocess_SERS`\n",
    "                        (`file_name`,`antibiotic`, `anti_conc`,`anti_time`, \n",
    "                         `data_path`,`count`, `combine_id`, `data_type` ,\n",
    "                         `cleaned`, `labeled`, `test` )\n",
    "               VALUES ('{file_name}','{antibiotic}', '{anti_conc}','{anti_time}', \n",
    "                       '{data_path}','{count}', '{combine_id}','{data_type}',\n",
    "                       '{cleaned}', '{labeled}','{test}');\"\"\".format(\n",
    "                file_name = 'SERS_testing.csv', antibiotic = antibiotic, \n",
    "                anti_conc=anti_conc, anti_time = anti_time, \n",
    "                data_path = data_dir + 'SERS_testing.csv', count = test_count,\n",
    "                combine_id = RAW_id, data_type = 'dataset', \n",
    "                cleaned = 1, labeled = 1, test = 1)\n",
    "# print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n",
    "## Record Train dataset into Database\n",
    "sql_command = \"\"\"INSERT INTO `Preprocess_SERS`\n",
    "                        (`file_name`,`antibiotic`, `anti_conc`,`anti_time`, \n",
    "                         `data_path`,`count`, `combine_id`, `data_type` ,\n",
    "                         `cleaned`, `labeled`, `train` )\n",
    "               VALUES ('{file_name}','{antibiotic}', '{anti_conc}','{anti_time}', \n",
    "                       '{data_path}','{count}', '{combine_id}','{data_type}',\n",
    "                       '{cleaned}', '{labeled}','{train}');\"\"\".format(\n",
    "                file_name = 'SERS_training.csv', antibiotic = antibiotic, \n",
    "                anti_conc=anti_conc, anti_time = anti_time, \n",
    "                data_path = data_dir + 'SERS_training.csv', count = train_count,\n",
    "                combine_id = RAW_id, data_type = 'dataset', \n",
    "                cleaned = 1, labeled = 1, train = 1)\n",
    "# print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "sql_command = \"\"\"SELECT `data_path` FROM `Preprocess_SERS`\n",
    "                WHERE `file_name` = '{file_name}';\"\"\".format(\n",
    "                    file_name = 'Cleaned_SERS_dataset.csv')\n",
    "cursor.execute(sql_command)\n",
    "# cursor.fetchall()\n",
    "cleaned_path = cursor.fetchall()[0][0]\n",
    "df = pd.read_csv(cleaned_path, header = 0)  ## data_path = data_dir + 'Cleaned_SERS_dataset.csv'\n",
    "\n",
    "#Normalization for each spectra\n",
    "label = df.pop('label')\n",
    "#df.pop('Unnamed: 0')\n",
    "header_name = df.columns\n",
    "df = df.to_numpy()\n",
    "min_value = np.min(df, axis=1)\n",
    "min_value = np.reshape(min_value, (-1, 1))\n",
    "min_value = np.repeat(min_value, df.shape[1], axis=1)\n",
    "max_value = np.max(df, axis=1)\n",
    "max_value = np.reshape(max_value, (-1, 1))\n",
    "max_value = np.repeat(max_value, df.shape[1], axis=1)\n",
    "df = df - min_value\n",
    "df = df / (max_value - min_value)\n",
    "#Find Peak\n",
    "df = pd.DataFrame(df, columns=header_name)\n",
    "df = pd.concat([label,df], axis=1)\n",
    "count = df.shape[0]\n",
    "df.to_csv(data_dir + 'Cleaned_Nor_SERS_dataset.csv', index= False)\n",
    "\n",
    "## Record Normalized dataset into Database\n",
    "sql_command = \"\"\"INSERT INTO `Preprocess_SERS`\n",
    "                        (`file_name`,`antibiotic`, `anti_conc`,`anti_time`, \n",
    "                         `data_path`,`count`, `combine_id`, `data_type` ,\n",
    "                         `cleaned`, `normalized` )\n",
    "               VALUES ('{file_name}','{antibiotic}', '{anti_conc}','{anti_time}', \n",
    "                       '{data_path}','{count}', '{combine_id}','{data_type}',\n",
    "                       '{cleaned}', '{normalized}');\"\"\".format(\n",
    "                file_name = 'Cleaned_Nor_SERS_dataset.csv', antibiotic = antibiotic, \n",
    "                anti_conc=anti_conc, anti_time = anti_time, \n",
    "                data_path = data_dir + 'Cleaned_Nor_SERS_dataset.csv', count = count,\n",
    "                combine_id = RAW_id, data_type = 'dataset', \n",
    "                cleaned = 1, normalized = 1)\n",
    "# print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n",
    "\n",
    "\n",
    "# df.groupby('label').mean()\n",
    "# df.groupby('label').std()\n",
    "\n",
    "\n",
    "cursor.execute(\"SELECT LAST_INSERT_ID();\")\n",
    "cleaned_nor_id = cursor.fetchall()[0][0]\n",
    "\n",
    "## Record mean of Normalized dataset into database\n",
    "df_mean= df.groupby('label').mean()\n",
    "df_mean.to_csv(data_dir + 'Cleaned_Nor_SERS_dataset_mean.csv', index= True)\n",
    "\n",
    "sql_command = \"\"\"INSERT INTO `Preprocess_SERS_Statistic`\n",
    "                        (`file_name`,`preprocess_id`, `data_type`,`data_path`)\n",
    "               VALUES ('{file_name}','{preprocess_id}', '{data_type}','{data_path}');\"\"\".format(\n",
    "                file_name = 'Cleaned_Nor_SERS_dataset_mean.csv', \n",
    "                preprocess_id = cleaned_nor_id,\n",
    "                data_type = 'mean',\n",
    "                data_path = data_dir + 'Cleaned_Nor_SERS_dataset_mean.csv')\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n",
    "\n",
    "## Record std of Normalized dataset into database\n",
    "df_std= df.groupby('label').std()\n",
    "df_std.to_csv(data_dir + 'Cleaned_Nor_SERS_dataset_std.csv', index= True)\n",
    "\n",
    "sql_command = \"\"\"INSERT INTO `Preprocess_SERS_Statistic`\n",
    "                        (`file_name`,`preprocess_id`, `data_type`,`data_path`)\n",
    "               VALUES ('{file_name}','{preprocess_id}', '{data_type}','{data_path}');\"\"\".format(\n",
    "                file_name = 'Cleaned_Nor_SERS_dataset_std.csv', \n",
    "                preprocess_id = cleaned_nor_id,\n",
    "                data_type = 'std',\n",
    "                data_path = data_dir + 'Cleaned_Nor_SERS_dataset_std.csv')\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized + label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nor label\n",
    "\n",
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "sql_command = \"\"\"SELECT `data_path` FROM `Preprocess_SERS`\n",
    "                WHERE `file_name` = '{file_name}';\"\"\".format(\n",
    "                    file_name = 'Cleaned_Nor_SERS_dataset.csv')\n",
    "cursor.execute(sql_command)\n",
    "# cursor.fetchall()\n",
    "cleaned_nor_path = cursor.fetchall()[0][0]\n",
    "\n",
    "\n",
    "df = pd.read_csv(cleaned_nor_path, header = 0) ## data_path = data_dir + 'Cleaned_Nor_SERS_dataset.csv'\n",
    "\n",
    "df['label'].replace({\n",
    "'Ecoli-ATCC27662_Amp16_30min': '0', \n",
    "'Ecoli-BL21_Amp16_30min': '1', \n",
    "'Ecoli-BW25113_Amp16_30min':'2', \n",
    "'Ecoli-DH5alpha_Amp16_30min':'3',   \n",
    "'Ecoli-DH5alpha-ampR_Amp16_30min':'4'}, inplace=True)\n",
    "\n",
    "df.to_csv(data_dir + 'Labeled_Nor_SERS_dataset.csv', index= False)\n",
    "\n",
    "## Record Lable dataset into Database\n",
    "sql_command = \"\"\"INSERT INTO `Preprocess_SERS`\n",
    "                        (`file_name`,`antibiotic`, `anti_conc`,`anti_time`, \n",
    "                         `data_path`,`count`, `combine_id`, `data_type` ,\n",
    "                         `cleaned`, `normalized`, `labeled` )\n",
    "               VALUES ('{file_name}','{antibiotic}', '{anti_conc}','{anti_time}', \n",
    "                       '{data_path}','{count}', '{combine_id}','{data_type}',\n",
    "                       '{cleaned}', '{normalized}','{labeled}');\"\"\".format(\n",
    "                file_name = 'Labeled_Nor_SERS_dataset.csv', antibiotic = antibiotic, \n",
    "                anti_conc=anti_conc, anti_time = anti_time, \n",
    "                data_path = data_dir + 'Labeled_Nor_SERS_dataset.csv', count = count,\n",
    "                combine_id = RAW_id, data_type = 'dataset', \n",
    "                cleaned = 1, normalized = 1,labeled = 1)\n",
    "# print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized: Train test dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nor: Train test dataset split\n",
    "\n",
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "## Directory\n",
    "data_dir = 'Exp_Data/'\n",
    "\n",
    "# experimental conditions\n",
    "antibiotic = 'Amp'\n",
    "anti_conc = 16\n",
    "anti_time = '30min'\n",
    "\n",
    "\n",
    "cursor.execute(\"\"\"SELECT `data_path`,`id` FROM `Combine_SERS`\n",
    "        WHERE (`antibiotic` = '{antibiotic}') AND (`anti_conc` = '{anti_conc}') AND (`anti_time` = '{anti_time}');\"\"\".format(\n",
    "        antibiotic = antibiotic, anti_conc = anti_conc, anti_time = anti_time))\n",
    "RAW_dataset = cursor.fetchall()\n",
    "RAW_file = RAW_dataset[0][0]\n",
    "RAW_id  = RAW_dataset[0][1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sql_command = \"\"\"SELECT `data_path` FROM `Preprocess_SERS`\n",
    "                WHERE `file_name` = '{file_name}';\"\"\".format(\n",
    "                    file_name = 'Labeled_Nor_SERS_dataset.csv')\n",
    "cursor.execute(sql_command)\n",
    "# cursor.fetchall()\n",
    "labeled_nor_path = cursor.fetchall()[0][0]\n",
    "\n",
    "\n",
    "df = pd.read_csv(labeled_nor_path, header = 0) ## data_path = data_dir + 'Labeled_Nor_SERS_dataset.csv'\n",
    "cols = list(df.columns)\n",
    "\n",
    "combine = df.to_numpy()\n",
    "np.random.seed(400)\n",
    "np.random.shuffle(combine)\n",
    "print (combine.shape)\n",
    "\n",
    "percent10 = int (combine.shape[0] * 0.1)\n",
    "print(percent10)\n",
    "\n",
    "test_set = combine[0:percent10,:]\n",
    "train_set = combine[percent10:,:]\n",
    "\n",
    "test_set = pd.DataFrame(test_set, columns=cols)\n",
    "train_set = pd.DataFrame(train_set, columns=cols)\n",
    "\n",
    "test_count = test_set.shape[0]\n",
    "train_count = train_set.shape[0]\n",
    "\n",
    "test_set.to_csv(data_dir  + 'SERS_Nor_testing.csv', index= False , header = True)\n",
    "train_set.to_csv(data_dir + 'SERS_Nor_training.csv', index= False , header = True)\n",
    "\n",
    "\n",
    "## Record Test Nor-dataset into Database\n",
    "sql_command = \"\"\"INSERT INTO `Preprocess_SERS`\n",
    "                        (`file_name`,`antibiotic`, `anti_conc`,`anti_time`, \n",
    "                         `data_path`,`count`, `combine_id`, `data_type` ,\n",
    "                         `cleaned`, `normalized`,`labeled`, `test` )\n",
    "               VALUES ('{file_name}','{antibiotic}', '{anti_conc}','{anti_time}', \n",
    "                       '{data_path}','{count}', '{combine_id}','{data_type}',\n",
    "                       '{cleaned}', '{normalized}','{labeled}','{test}');\"\"\".format(\n",
    "                file_name = 'SERS_Nor_testing.csv', antibiotic = antibiotic, \n",
    "                anti_conc=anti_conc, anti_time = anti_time, \n",
    "                data_path = data_dir + 'SERS_Nor_testing.csv', count = test_count,\n",
    "                combine_id = RAW_id, data_type = 'dataset', \n",
    "                cleaned = 1, normalized = 1, labeled = 1, test = 1)\n",
    "print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Record Train Nor-dataset into Database\n",
    "sql_command = \"\"\"INSERT INTO `Preprocess_SERS`\n",
    "                        (`file_name`,`antibiotic`, `anti_conc`,`anti_time`, \n",
    "                         `data_path`,`count`, `combine_id`, `data_type` ,\n",
    "                         `cleaned`, `normalized`, `labeled`, `train` )\n",
    "               VALUES ('{file_name}','{antibiotic}', '{anti_conc}','{anti_time}', \n",
    "                       '{data_path}','{count}', '{combine_id}','{data_type}',\n",
    "                       '{cleaned}', '{normalized}', '{labeled}','{train}');\"\"\".format(\n",
    "                file_name = 'SERS_Nor_training.csv', antibiotic = antibiotic, \n",
    "                anti_conc=anti_conc, anti_time = anti_time, \n",
    "                data_path = data_dir + 'SERS_Nor_training.csv', count = train_count,\n",
    "                combine_id = RAW_id, data_type = 'dataset', \n",
    "                cleaned = 1, normalized = 1, labeled = 1, train = 1)\n",
    "# print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "899abd191a9ea73a89afb19849dc406f29b830785484ec2528e59ce4ca659ec1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
