{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensional Reduction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn import manifold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "from time import time\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import mysql.connector\n",
    "import pymysql\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mydb = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"********\"\n",
    "    )\n",
    "    print(\"Connection established\")\n",
    "    cursor = mydb.cursor(buffered=True)\n",
    "    cursor.execute(\"use `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(\"An error occurred:\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dir = input(\"Please enter the directory: \\n\")\n",
    "\n",
    "input_dir = 'Exp_Data/'\n",
    "output_dir = 'Exp_Result/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "training_sizes = [50,100,200,400,800,1600,3200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ensure database connection\n",
    "\n",
    "# from sqlalchemy import create_engine\n",
    "engine = create_engine('mysql+pymysql://root:********@localhost:3306/SERS_ML_TEST')\n",
    "\n",
    "\n",
    "xlabel_General = np.array([ 'ATCC 27662_Amp16','BL21_Amp16', 'BW25113_Amp16', 'DH5\\u03B1(WT)_Amp16','DH5\\u03B1(ampR)_Amp16']) \n",
    "x_General = np.array([0, 1, 2, 3, 4])\n",
    "color_map = [ 'darkorange','darkviolet','navy','red','darkgreen']\n",
    "marker_list = ['o', 'o', 'o','o','o' ]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(np.array([x_General,xlabel_General,color_map, marker_list]).T, columns = ['id', 'label', 'color_map', 'marker_list'])\n",
    "df.to_sql('Visual_Labels', engine, index=False, if_exists='append')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Data_Visualization into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "\n",
    "sql_command = \"\"\"INSERT INTO `Visualize_Methods`(`method`)\n",
    "               VALUES ('PCA'),('T_SNE');\"\"\"\n",
    "\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "sql_command = \"\"\"SELECT `data_path`, `id` FROM `Preprocess_SERS`\n",
    "                WHERE `file_name` = '{file_name}';\"\"\".format(\n",
    "                    file_name = 'Labeled_SERS_dataset.csv')\n",
    "cursor.execute(sql_command)\n",
    "labeled_info =  cursor.fetchall()\n",
    "labeled_path = labeled_info[0][0]\n",
    "labeled_id   = labeled_info[0][1]\n",
    "\n",
    "\n",
    "df = pd.read_csv(labeled_path, header = 0)  # input_dir + 'Labeled_SERS_dataset.csv'\n",
    "\n",
    "# Sorting by Label\n",
    "df.sort_values(by = 'label', inplace=True)\n",
    "\n",
    "\n",
    "plt.rcParams['font.size'] = 6\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "feature = df.loc[:, '400.0':'1550.0']\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_feature = pca.fit_transform(feature)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "temp_label = 0\n",
    "for i in range(pca_feature.shape[0]):  \n",
    "    if temp_label == 0 and int(df.iloc[i, 0]) == 0:\n",
    "        plt.scatter(pca_feature[i,0], pca_feature[i,1],c = color_map[int(df.iloc[i, 0])], label = xlabel_General[int(df.iloc[i, 0])], alpha = 0.5, s = 10, marker = marker_list[int(df.iloc[i, 0])], edgecolors = None)\n",
    "        temp_label = temp_label + 1\n",
    "    elif temp_label == int(df.iloc[i, 0]) and (temp_label != 0):\n",
    "        plt.scatter(pca_feature[i,0], pca_feature[i,1],c = color_map[int(df.iloc[i, 0])], label = xlabel_General[int(df.iloc[i, 0])], alpha = 0.5, s = 10, marker = marker_list[int(df.iloc[i, 0])], edgecolors = None)\n",
    "        temp_label = temp_label + 1\n",
    "        print( xlabel_General[int(df.iloc[i, 0])])\n",
    "        print( int(df.iloc[i, 0]) )\n",
    "    else :\n",
    "        plt.scatter(pca_feature[i,0], pca_feature[i,1],c = color_map[int(df.iloc[i, 0])], alpha = 0.5, s = 10, marker = marker_list[int(df.iloc[i, 0])], edgecolors = None)\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "plt.legend(reversed(handles), reversed(labels),bbox_to_anchor=(1.1, 1), loc='upper left')\n",
    "\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.axis('square') \n",
    "plt.savefig(output_dir + 'Labeled_SERS_PCA.png', dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "## Generate PCA dataset \n",
    "pca_dataframe = pd.DataFrame(pca_feature, columns=['1st_D', '2nd_D'])\n",
    "pca_dataframe.insert(0, 'label', df.iloc[:, 0])\n",
    "print(pca_dataframe)\n",
    "pca_dataframe.to_csv(output_dir + 'PCA_dataset.csv', index= False)\n",
    "\n",
    "\n",
    "\n",
    "## Record PCA Figure & dataset into Database\n",
    "sql_command = \"\"\"INSERT INTO `Data_Visualization`\n",
    "                        (`method`,`method_id`, `dataset_id`,`dataset_path`, \n",
    "                         `method_parameters_id`,`normalized`,\n",
    "                           `result_path`, `figure_path`)\n",
    "               VALUES ('{method}','{method_id}', '{dataset_id}','{dataset_path}', \n",
    "                       '{method_parameters_id}','{normalized}',\n",
    "                       '{result_path}', '{figure_path}');\"\"\".format(\n",
    "                method = 'PCA', method_id = '1', \n",
    "                dataset_id = labeled_id, dataset_path = labeled_path, \n",
    "                method_parameters_id = '1', normalized = 0,\n",
    "                result_path = output_dir + 'PCA_dataset.csv', figure_path = output_dir + 'Labeled_SERS_PCA.png')\n",
    "# print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "sql_command = \"\"\"SELECT `data_path`, `id` FROM `Preprocess_SERS`\n",
    "                WHERE `file_name` = '{file_name}';\"\"\".format(\n",
    "                    file_name = 'Labeled_Nor_SERS_dataset.csv')\n",
    "cursor.execute(sql_command)\n",
    "labeled_nor_info =  cursor.fetchall()\n",
    "labeled_nor_path = labeled_nor_info[0][0]\n",
    "labeled_nor_id   = labeled_nor_info[0][1]\n",
    "\n",
    "## Normalized PCA\n",
    "df = pd.read_csv(labeled_nor_path, header = 0)  # input_dir + 'Labeled_Nor_SERS_dataset.csv'\n",
    "# Sorting by Label\n",
    "df.sort_values(by = 'label', inplace=True)\n",
    "\n",
    "\n",
    "plt.rcParams['font.size'] = 6\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "feature = df.loc[:, '400.0':'1550.0']\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_feature = pca.fit_transform(feature)\n",
    "plt.figure()\n",
    "temp_label = 0\n",
    "for i in range(pca_feature.shape[0]):  \n",
    "    if temp_label == 0 and int(df.iloc[i, 0]) == 0:\n",
    "        plt.scatter(pca_feature[i,0], pca_feature[i,1],c = color_map[int(df.iloc[i, 0])], label = xlabel_General[int(df.iloc[i, 0])], alpha = 0.5, s = 10, marker = marker_list[int(df.iloc[i, 0])], edgecolors = None)\n",
    "        temp_label = temp_label + 1\n",
    "    elif temp_label == int(df.iloc[i, 0]) and (temp_label != 0):\n",
    "        plt.scatter(pca_feature[i,0], pca_feature[i,1],c = color_map[int(df.iloc[i, 0])], label = xlabel_General[int(df.iloc[i, 0])], alpha = 0.5, s = 10, marker = marker_list[int(df.iloc[i, 0])], edgecolors = None)\n",
    "        temp_label = temp_label + 1\n",
    "        print( xlabel_General[int(df.iloc[i, 0])])\n",
    "        print( int(df.iloc[i, 0]) )\n",
    "    else :\n",
    "        plt.scatter(pca_feature[i,0], pca_feature[i,1],c = color_map[int(df.iloc[i, 0])], alpha = 0.5, s = 10, marker = marker_list[int(df.iloc[i, 0])], edgecolors = None)\n",
    "\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "plt.legend(reversed(handles), reversed(labels),bbox_to_anchor=(1.1, 1), loc='upper left')\n",
    "\n",
    "# plt.xlim([-50000, 100000])\n",
    "# plt.ylim([-50000, 50000])\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.axis('square') \n",
    "plt.savefig(output_dir + 'Labeled_Nor_SERS_PCA.png', dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "## Generate PCA dataset \n",
    "pca_dataframe = pd.DataFrame(pca_feature, columns=['1st_D', '2nd_D'])\n",
    "pca_dataframe.insert(0, 'label', df.iloc[:, 0])\n",
    "print(pca_dataframe)\n",
    "pca_dataframe.to_csv(output_dir + 'Nor_PCA_dataset.csv', index= False)\n",
    "\n",
    "\n",
    "\n",
    "## Record PCA Figure & dataset into Database\n",
    "sql_command = \"\"\"INSERT INTO `Data_Visualization`\n",
    "                        (`method`,`method_id`, `dataset_id`,`dataset_path`, \n",
    "                         `method_parameters_id`,`normalized`,\n",
    "                           `result_path`, `figure_path`)\n",
    "               VALUES ('{method}','{method_id}', '{dataset_id}','{dataset_path}', \n",
    "                       '{method_parameters_id}','{normalized}',\n",
    "                       '{result_path}', '{figure_path}');\"\"\".format(\n",
    "                method = 'PCA', method_id = '1', \n",
    "                dataset_id = labeled_nor_id, dataset_path = labeled_nor_path, \n",
    "                method_parameters_id = '1', normalized = 1,\n",
    "                result_path = output_dir + 'Nor_PCA_dataset.csv', figure_path = output_dir + 'Labeled_Nor_SERS_PCA.png')\n",
    "# print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "sql_command = \"\"\"SELECT `data_path`, `id` FROM `Preprocess_SERS`\n",
    "                WHERE `file_name` = '{file_name}';\"\"\".format(\n",
    "                    file_name = 'Labeled_SERS_dataset.csv')\n",
    "cursor.execute(sql_command)\n",
    "labeled_info =  cursor.fetchall()\n",
    "labeled_path = labeled_info[0][0]\n",
    "labeled_id   = labeled_info[0][1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(labeled_path, header = 0)  # input_dir + 'Labeled_SERS_dataset.csv'\n",
    "# Sorting by Label\n",
    "df.sort_values(by = 'label', inplace=True)\n",
    "\n",
    "\n",
    "plt.rcParams['font.size'] = 6\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "feature = df.loc[:, '400.0':'1550.0']\n",
    "\n",
    "\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, init='random', learning_rate=200, perplexity = 50).fit_transform(feature)\n",
    "x_min, x_max = tsne.min(0), tsne.max(0)\n",
    "tsne = (tsne - x_min) / (x_max - x_min)\n",
    "\n",
    "plt.figure()\n",
    "temp_label = 0\n",
    "for i in range(tsne.shape[0]):\n",
    "    if temp_label == 0 and int(df.iloc[i, 0]) == 0:\n",
    "        plt.scatter(tsne[i,0], tsne[i,1],c = color_map[int(df.iloc[i, 0])], label = xlabel_General[int(df.iloc[i, 0])], alpha = 0.5, s=10, marker = marker_list[int(df.iloc[i, 0])])\n",
    "        temp_label = temp_label + 1\n",
    "    elif temp_label == int(df.iloc[i, 0]) and temp_label != 0:\n",
    "        plt.scatter(tsne[i,0], tsne[i,1],c = color_map[int(df.iloc[i, 0])], label = xlabel_General[int(df.iloc[i, 0])], alpha = 0.5, s=10, marker = marker_list[int(df.iloc[i, 0])])\n",
    "        temp_label = temp_label + 1\n",
    "    else :\n",
    "        plt.scatter(tsne[i,0], tsne[i,1],c = color_map[int(df.iloc[i, 0])], alpha = 0.5, s=10 ,marker = marker_list[int(df.iloc[i, 0])])\n",
    "\n",
    "\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "plt.legend(reversed(handles), reversed(labels),bbox_to_anchor=(1.1, 1), loc='upper left')\n",
    "plt.xlabel('T-SNE1')\n",
    "plt.ylabel('T-SNE2')\n",
    "plt.axis('square')\n",
    "plt.savefig(output_dir + 'Labeled_SERS_t-SNE.png', dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "## Generate T-SNE dataset \n",
    "tsne_dataframe = pd.DataFrame(tsne, columns=['1st_D', '2nd_D'])\n",
    "tsne_dataframe.insert(0, 'label', df.iloc[:, 0])\n",
    "print(tsne_dataframe)\n",
    "tsne_dataframe.to_csv(output_dir + 't-SNE_dataset.csv', index= False)\n",
    "\n",
    "\n",
    "\n",
    "## Record PCA Figure & dataset into Database\n",
    "sql_command = \"\"\"INSERT INTO `Data_Visualization`\n",
    "                        (`method`,`method_id`, `dataset_id`,`dataset_path`, \n",
    "                         `method_parameters_id`,`normalized`,\n",
    "                           `result_path`, `figure_path`)\n",
    "               VALUES ('{method}','{method_id}', '{dataset_id}','{dataset_path}', \n",
    "                       '{method_parameters_id}','{normalized}',\n",
    "                       '{result_path}', '{figure_path}');\"\"\".format(\n",
    "                method = 'T-SNE', method_id = '2', \n",
    "                dataset_id = labeled_id, dataset_path = labeled_path, \n",
    "                method_parameters_id = '1', normalized = 0,\n",
    "                result_path = output_dir + 't-SNE_dataset.csv', figure_path = output_dir + 'Labeled_SERS_t-SNE.png')\n",
    "# print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "sql_command = \"\"\"SELECT `data_path`, `id` FROM `Preprocess_SERS`\n",
    "                WHERE `file_name` = '{file_name}';\"\"\".format(\n",
    "                    file_name = 'Labeled_Nor_SERS_dataset.csv')\n",
    "cursor.execute(sql_command)\n",
    "labeled_nor_info =  cursor.fetchall()\n",
    "labeled_nor_path = labeled_nor_info[0][0]\n",
    "labeled_nor_id   = labeled_nor_info[0][1]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(labeled_nor_path, header = 0) ## input_dir + 'Labeled_Nor_SERS_dataset.csv'\n",
    "\n",
    "\n",
    "df.sort_values(by = 'label', inplace=True)\n",
    "\n",
    "plt.rcParams['font.size'] = 6\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "feature = df.loc[:, '400.0':'1550.0']\n",
    "\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, init='random', learning_rate=200, perplexity = 50).fit_transform(feature)\n",
    "x_min, x_max = tsne.min(0), tsne.max(0)\n",
    "tsne = (tsne - x_min) / (x_max - x_min)\n",
    "\n",
    "plt.figure()\n",
    "temp_label = 0\n",
    "for i in range(tsne.shape[0]):\n",
    "    if temp_label == 0 and int(df.iloc[i, 0]) == 0:\n",
    "        plt.scatter(tsne[i,0], tsne[i,1],c = color_map[int(df.iloc[i, 0])], label = xlabel_General[int(df.iloc[i, 0])], alpha = 0.5, s=10, marker = marker_list[int(df.iloc[i, 0])])\n",
    "        temp_label = temp_label + 1\n",
    "    elif temp_label == int(df.iloc[i, 0]) and temp_label != 0:\n",
    "        plt.scatter(tsne[i,0], tsne[i,1],c = color_map[int(df.iloc[i, 0])], label = xlabel_General[int(df.iloc[i, 0])], alpha = 0.5, s=10, marker = marker_list[int(df.iloc[i, 0])])\n",
    "        temp_label = temp_label + 1\n",
    "    else :\n",
    "        plt.scatter(tsne[i,0], tsne[i,1],c = color_map[int(df.iloc[i, 0])], alpha = 0.5, s=10 ,marker = marker_list[int(df.iloc[i, 0])])\n",
    "\n",
    "\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "plt.legend(reversed(handles), reversed(labels),bbox_to_anchor=(1.1, 1), loc='upper left')\n",
    "plt.xlabel('T-SNE1')\n",
    "plt.ylabel('T-SNE2')\n",
    "plt.axis('square')\n",
    "plt.savefig(output_dir + 'Labeled_Nor_SERS_t-SNE.png', dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "## Generate T-SNE dataset \n",
    "tsne_dataframe = pd.DataFrame(tsne, columns=['1st_D', '2nd_D'])\n",
    "tsne_dataframe.insert(0, 'label', df.iloc[:, 0])\n",
    "print(tsne_dataframe)\n",
    "tsne_dataframe.to_csv(output_dir + 'Nor_t-SNE_dataset.csv', index= False)\n",
    "\n",
    "\n",
    "\n",
    "## Record PCA Figure & dataset into Database\n",
    "sql_command = \"\"\"INSERT INTO `Data_Visualization`\n",
    "                        (`method`,`method_id`, `dataset_id`,`dataset_path`, \n",
    "                         `method_parameters_id`,`normalized`,\n",
    "                           `result_path`, `figure_path`)\n",
    "               VALUES ('{method}','{method_id}', '{dataset_id}','{dataset_path}', \n",
    "                       '{method_parameters_id}','{normalized}',\n",
    "                       '{result_path}', '{figure_path}');\"\"\".format(\n",
    "                method = 'T-SNE', method_id = '2', \n",
    "                dataset_id = labeled_nor_id, dataset_path = labeled_nor_path, \n",
    "                method_parameters_id = '1', normalized = 1,\n",
    "                result_path = output_dir + 'Nor_t-SNE_dataset.csv', figure_path = output_dir + 'Labeled_Nor_SERS_t-SNE.png')\n",
    "# print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE perplexity = 500, Normalized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nor: perplexity 500\n",
    "\n",
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "sql_command = \"\"\"SELECT `data_path`, `id` FROM `Preprocess_SERS`\n",
    "                WHERE `file_name` = '{file_name}';\"\"\".format(\n",
    "                    file_name = 'Labeled_Nor_SERS_dataset.csv')\n",
    "cursor.execute(sql_command)\n",
    "labeled_nor_info =  cursor.fetchall()\n",
    "labeled_nor_path = labeled_nor_info[0][0]\n",
    "labeled_nor_id   = labeled_nor_info[0][1]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(labeled_nor_path, header = 0) ## input_dir + 'Labeled_Nor_SERS_dataset.csv'\n",
    "df.sort_values(by = 'label', inplace=True)\n",
    "\n",
    "plt.rcParams['font.size'] = 6\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "feature = df.loc[:, '400.0':'1550.0']\n",
    "\n",
    "\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, init='random', learning_rate=200, perplexity = 500).fit_transform(feature)\n",
    "x_min, x_max = tsne.min(0), tsne.max(0)\n",
    "tsne = (tsne - x_min) / (x_max - x_min)\n",
    "\n",
    "plt.figure()\n",
    "temp_label = 0\n",
    "for i in range(tsne.shape[0]):\n",
    "    if temp_label == 0 and int(df.iloc[i, 0]) == 0:\n",
    "        plt.scatter(tsne[i,0], tsne[i,1],c = color_map[int(df.iloc[i, 0])], label = xlabel_General[int(df.iloc[i, 0])], alpha = 0.5, s=10, marker = marker_list[int(df.iloc[i, 0])])\n",
    "        temp_label = temp_label + 1\n",
    "    elif temp_label == int(df.iloc[i, 0]) and temp_label != 0:\n",
    "        plt.scatter(tsne[i,0], tsne[i,1],c = color_map[int(df.iloc[i, 0])], label = xlabel_General[int(df.iloc[i, 0])], alpha = 0.5, s=10, marker = marker_list[int(df.iloc[i, 0])])\n",
    "        temp_label = temp_label + 1\n",
    "    else :\n",
    "        plt.scatter(tsne[i,0], tsne[i,1],c = color_map[int(df.iloc[i, 0])], alpha = 0.5, s=10 ,marker = marker_list[int(df.iloc[i, 0])])\n",
    "\n",
    "\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "plt.legend(reversed(handles), reversed(labels),bbox_to_anchor=(1.1, 1), loc='upper left')\n",
    "plt.xlabel('T-SNE1')\n",
    "plt.ylabel('T-SNE2')\n",
    "plt.axis('square')\n",
    "plt.savefig(output_dir + 'Labeled_Nor_SERS_t-SNE_p500.png', dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "## Generate T-SNE dataset \n",
    "tsne_dataframe = pd.DataFrame(tsne, columns=['1st_D', '2nd_D'])\n",
    "tsne_dataframe.insert(0, 'label', df.iloc[:, 0])\n",
    "print(tsne_dataframe)\n",
    "tsne_dataframe.to_csv(output_dir + 'Nor_t-SNE_p500_dataset.csv', index= False)\n",
    "\n",
    "\n",
    "\n",
    "## Record PCA Figure & dataset into Database\n",
    "sql_command = \"\"\"INSERT INTO `Data_Visualization`\n",
    "                        (`method`,`method_id`, `dataset_id`,`dataset_path`, \n",
    "                         `method_parameters_id`,`normalized`,\n",
    "                           `result_path`, `figure_path`)\n",
    "               VALUES ('{method}','{method_id}', '{dataset_id}','{dataset_path}', \n",
    "                       '{method_parameters_id}','{normalized}',\n",
    "                       '{result_path}', '{figure_path}');\"\"\".format(\n",
    "                method = 'T-SNE', method_id = '2', \n",
    "                dataset_id = labeled_nor_id, dataset_path = labeled_nor_path, \n",
    "                method_parameters_id = '2', normalized = 1,\n",
    "                result_path = output_dir + 'Nor_t-SNE_p500_dataset.csv', figure_path = output_dir + 'Labeled_Nor_SERS_t-SNE_p500.png')\n",
    "# print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine learning classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of ML methods into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "\n",
    "sql_command = \"\"\"INSERT INTO `ML_methods`(`method`, `method_fullname`)\n",
    "               VALUES ('RF',   'Random forest'),\n",
    "                      ('SVM',  'Support vector machine'),\n",
    "                      ('KNN',  'k-nearest neighbors'),\n",
    "                      ('CNN',  'Convolutional neural network');\"\"\"\n",
    "\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing Set info collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "## Training_Set path and id\n",
    "sql_command = \"\"\"SELECT `data_path`, `id`, `count` FROM `Preprocess_SERS`\n",
    "                WHERE (`normalized` = '1')\n",
    "                AND (`train` = '1')\n",
    "                AND (`test` = '0');\"\"\"\n",
    "cursor.execute(sql_command)\n",
    "train_set_info =  cursor.fetchall()\n",
    "print(train_set_info)\n",
    "train_set_path = train_set_info[0][0]\n",
    "train_set_id   = train_set_info[0][1]\n",
    "train_set_size   = train_set_info[0][2]\n",
    "\n",
    "## Test_Set path and id\n",
    "sql_command = \"\"\"SELECT `data_path`, `id`,`count`  FROM `Preprocess_SERS`\n",
    "                WHERE (`normalized` = '1')\n",
    "                AND (`train` = '0')\n",
    "                AND (`test` = '1');\"\"\"\n",
    "cursor.execute(sql_command)\n",
    "test_set_info =  cursor.fetchall()\n",
    "print(test_set_info)\n",
    "test_set_path = test_set_info[0][0]\n",
    "test_set_id   = test_set_info[0][1]\n",
    "test_set_size   = test_set_info[0][2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML training: RF, SVM, KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(train_set_path,header = 0)  ## input_dir + 'SERS_Nor_training.csv'\n",
    "# feature = df.loc[:, '400.0':'1550.0']\n",
    "feature = df.iloc[:, 1:].to_numpy()\n",
    "# print(feature)\n",
    "train_label = df['label'].to_numpy()\n",
    "\n",
    "## Model Training\n",
    "#Random Forest\n",
    "rf = RandomForestClassifier(max_depth=40, max_samples=1.0, min_samples_split=5,random_state=0)\n",
    "rf.fit(feature, train_label)\n",
    "\n",
    "#SVM\n",
    "svm = SVC(C=10)\n",
    "svm.fit(feature, train_label)\n",
    "\n",
    "#KNN\n",
    "knn = KNeighborsClassifier(algorithm='brute', n_neighbors=10, weights='distance')\n",
    "knn.fit(feature, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Training: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "Epoch = 200\n",
    "BATCH_SIZE = 200\n",
    "learning_rate = 0.0001 \n",
    "wd=0.00001\n",
    "\n",
    "df = pd.read_csv(train_set_path,header = 0)  ## input_dir + 'SERS_Nor_training.csv'\n",
    "\n",
    "combine = df.to_numpy()\n",
    "np.random.seed(8787)\n",
    "np.random.shuffle(combine)\n",
    "# feature = combine[:, 1:]\n",
    "# label =  combine[:, 0]\n",
    "feature_train = combine[:, 1:]\n",
    "label_train =  combine[:, 0]\n",
    "train_size = int(feature_train.shape[0] * 1)\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(test_set_path, header = 0) ## input_dir + 'SERS_Nor_testing.csv'\n",
    "# combine = df_test.loc[:, 400.0:'Label'].to_numpy()\n",
    "combine = df_test.to_numpy()\n",
    "np.random.seed(8787)\n",
    "np.random.shuffle(combine)\n",
    "feature_test = combine[:, 1:]\n",
    "label_test =  combine[:, 0]\n",
    "test_size = int(feature_test.shape[0] * 1)\n",
    "\n",
    "\n",
    "\n",
    "#Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 6, 3, 2)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.bn1 = nn.BatchNorm1d(6)\n",
    "        self.conv2 = nn.Conv1d(6, 16, 3, 2)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.fc1 = nn.Linear(2288, 280)\n",
    "        self.fc2 = nn.Linear(280, 14)\n",
    "        self.fc3 = nn.Linear(14, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape((x.shape[0],1,-1))\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "feature_train = torch.from_numpy(feature_train)\n",
    "label_train = torch.from_numpy(label_train)\n",
    "feature_test = torch.from_numpy(feature_test)\n",
    "label_test = torch.from_numpy(label_test)\n",
    "\n",
    "train_dataset = Data.TensorDataset(feature_train, label_train)\n",
    "test_dataset = Data.TensorDataset(feature_test, label_test)\n",
    "train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "#Model Setting\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU run\")\n",
    "cnn_model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=learning_rate, weight_decay=wd)\n",
    "\n",
    "\n",
    "#Model Training\n",
    "cnn_model_path = output_dir + 'SERS_Nor_CNN.pth'\n",
    "\n",
    "\n",
    "accuracy_record = {'train': [], 'test': []} \n",
    "loss_record = {'train': [], 'test': []} \n",
    "best_train_acc = 0.0\n",
    "best_train_loss = 0.0\n",
    "\n",
    "\n",
    "initial_time = time()\n",
    "\n",
    "for epoch in range(Epoch):  # loop over the dataset multiple times\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    cnn_model.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.type(torch.FloatTensor)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn_model(inputs)\n",
    "        loss =  criterion(outputs, labels)\n",
    "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "\n",
    "    accuracy_record['train'].append(train_acc/len(train_dataset))\n",
    "    loss_record['train'].append(train_loss/len(train_loader))    \n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:    # print every 2000 mini-batches\n",
    "        print(f'{epoch + 1}, train_loss: {train_loss /len(train_loader)}, train_acc: {train_acc/len(train_dataset)}')\n",
    "\n",
    "    if train_acc > best_train_acc:\n",
    "        best_train_acc = train_acc\n",
    "        print('[Save]-- [{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "            epoch + 1, Epoch, train_acc/len(train_dataset), train_loss/len(train_loader)\n",
    "            ))\n",
    "\n",
    "\n",
    "    cnn_model.eval() # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.type(torch.FloatTensor)\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = cnn_model(inputs)\n",
    "            loss =  criterion(outputs, labels)\n",
    "            _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "            test_acc += (test_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        accuracy_record['test'].append(test_acc/len(test_dataset))\n",
    "        loss_record['test'].append(test_loss/len(test_loader))\n",
    "\n",
    "\n",
    "\n",
    "torch.save(cnn_model.state_dict(), cnn_model_path)\n",
    "\n",
    "print('Finished Training (02)')\n",
    "print('Training time', time() - initial_time)\n",
    "\n",
    "\n",
    "acc_pd = pd.DataFrame.from_dict(accuracy_record)\n",
    "loss_pd = pd.DataFrame.from_dict(loss_record)\n",
    "lc_pd = df = pd.concat([acc_pd,loss_pd], axis=1)\n",
    "lc_filename =  output_dir + 'SERS_Nor_CNN_learnCurve.csv'\n",
    "lc_pd.to_csv(lc_filename, index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML prediction: RF, SVM, KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(test_set_path, header = 0) ## input_dir + 'SERS_Nor_testing.csv'\n",
    "feature = df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "\n",
    "label = df['label'].to_numpy()\n",
    "label = torch.from_numpy(label)\n",
    "\n",
    "#Random Forest\n",
    "rf_result = rf.predict(feature)\n",
    "rf_result_df = pd.DataFrame(rf_result)\n",
    "rf_result_df.to_csv(output_dir  + 'Nor_RF_prediction.csv', index= True , header = False)\n",
    "rf_result = torch.from_numpy(rf_result)\n",
    "\n",
    "#SVM\n",
    "svm_result = svm.predict(feature)\n",
    "svm_result_df = pd.DataFrame(svm_result)\n",
    "svm_result_df.to_csv(output_dir  + 'Nor_SVM_prediction.csv', index= True , header = False)\n",
    "# print(svm_result_df)\n",
    "svm_result = torch.from_numpy(svm_result)\n",
    "\n",
    "#KNN\n",
    "knn_result = knn.predict(feature)\n",
    "knn_result_df = pd.DataFrame(knn_result)\n",
    "knn_result_df.to_csv(output_dir  + 'Nor_KNN_prediction.csv', index= True , header = False)\n",
    "knn_result = torch.from_numpy(knn_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML prediction: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "from time import time\n",
    "\n",
    "df = pd.read_csv(test_set_path, header = 0) ## input_dir + 'SERS_Nor_testing.csv'\n",
    "\n",
    "# feature = df.loc[:, 400.0:1550.0].to_numpy()\n",
    "feature = df.loc[:, '400.0':'1550.0'].to_numpy()\n",
    "feature = torch.from_numpy(feature)\n",
    "label = df['label'].to_numpy()\n",
    "label = torch.from_numpy(label)\n",
    "\n",
    "\n",
    "#Data_Loader\n",
    "batch_num = feature.shape[0]\n",
    "dataset = Data.TensorDataset(feature, label)\n",
    "test_loader = Data.DataLoader(dataset, batch_size=batch_num)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 6, 3, 2)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.bn1 = nn.BatchNorm1d(6)\n",
    "        self.conv2 = nn.Conv1d(6, 16, 3, 2)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.fc1 = nn.Linear(2288, 280)\n",
    "        self.fc2 = nn.Linear(280, 14)\n",
    "        self.fc3 = nn.Linear(14, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape((x.shape[0],1,-1))\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load(output_dir + 'SERS_Nor_CNN.pth'))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pred_acc = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        features, labels = data\n",
    "        features = features.type(torch.FloatTensor)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        features = features.reshape((features.shape[0],1, 1, -1))\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(features)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred_acc += (predicted.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "        cnn_result = predicted.to('cpu')\n",
    "\n",
    "\n",
    "cnn_result = cnn_result.cpu().numpy()\n",
    "cnn_result_df = pd.DataFrame(cnn_result)\n",
    "cnn_result_df.to_csv(output_dir  + 'Nor_CNN_prediction.csv', index= True , header = False)\n",
    "cnn_result = torch.from_numpy(cnn_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_calculation(predict, label):\n",
    "    correct = (predict == label).sum().item()\n",
    "    total = label.size(0) \n",
    "    return round(100 * correct / total, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#RF\n",
    "rf_acc = acc_calculation(rf_result, label)\n",
    "\n",
    "#SVM\n",
    "svm_acc = acc_calculation(svm_result, label)\n",
    "\n",
    "#KNN\n",
    "knn_acc = acc_calculation(knn_result, label)\n",
    "\n",
    "#CNN\n",
    "cnn_acc = acc_calculation(cnn_result, label)\n",
    "\n",
    "\n",
    "print(f'Random Forest ACC: {rf_acc}% \\n SVM ACC: {svm_acc}% \\n KNN ACC: {knn_acc}% \\n CNN ACC: {cnn_acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Prediction record as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_acc = pd.DataFrame([['Random Forest', rf_acc],['SVM' , svm_acc],['KNN', knn_acc], ['CNN', cnn_acc]], columns=['method', 'accuracy(%)'])\n",
    "print(ml_acc)\n",
    "ml_acc.to_csv(output_dir  + 'Nor_ML_ACC.csv', index= False , header = True)\n",
    "\n",
    "# print(ml_acc.iloc[0,:]) \n",
    "print(ml_acc.iloc[0,:]) \n",
    "\n",
    "ml_acc.iloc[0,:].to_csv(output_dir  + 'Nor_RF_ACC.csv', index= True , header = False)\n",
    "ml_acc.iloc[1,:].to_csv(output_dir  + 'Nor_SVM_ACC.csv', index= True , header = False)\n",
    "ml_acc.iloc[2,:].to_csv(output_dir  + 'Nor_KNN_ACC.csv', index= True , header = False)\n",
    "ml_acc.iloc[3,:].to_csv(output_dir  + 'Nor_CNN_ACC.csv', index= True , header = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix plot & record ML results into database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 8\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "\n",
    "xlabel_General = np.array([ 'ATCC 27662_Amp16','BL21_Amp16', 'BW25113_Amp16', 'DH5\\u03B1(WT)_Amp16','DH5\\u03B1(ampR)_Amp16']) \n",
    "x_General = np.array([0, 1, 2, 3, 4])   \n",
    "\n",
    "\n",
    "#RF\n",
    "plt.figure(1)\n",
    "rf_con = confusion_matrix(label, rf_result,normalize='true')\n",
    "rf_con_df = pd.DataFrame(rf_con)\n",
    "rf_con_df.to_csv(output_dir  + 'Nor_RF_conf.csv', index= False , header = False)\n",
    "\n",
    "rf_con = np.around(rf_con, 2)\n",
    "rf_disp = ConfusionMatrixDisplay(confusion_matrix=rf_con, display_labels= xlabel_General)\n",
    "rf_disp.plot(cmap ='gist_yarg', colorbar=False)\n",
    "plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "plt.savefig(output_dir + 'Nor_RF_conf.png', dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "#SVM\n",
    "plt.figure(2)\n",
    "svm_con = confusion_matrix(label, svm_result,normalize='true')\n",
    "svm_con_df = pd.DataFrame(svm_con)\n",
    "svm_con_df.to_csv(output_dir  + 'Nor_SVM_conf.csv', index= False , header = False)\n",
    "\n",
    "svm_con = np.around(svm_con, 2)\n",
    "svm_disp = ConfusionMatrixDisplay(confusion_matrix=svm_con,display_labels= xlabel_General)\n",
    "svm_disp.plot(cmap ='gist_yarg', colorbar=False)\n",
    "plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "plt.savefig(output_dir + 'Nor_SVM_conf.png', dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "#KNN\n",
    "plt.figure(3)\n",
    "knn_con = confusion_matrix(label, knn_result,normalize='true')\n",
    "knn_con_df = pd.DataFrame(knn_con)\n",
    "knn_con_df.to_csv(output_dir  + 'Nor_KNN_conf.csv', index= False , header = False)\n",
    "\n",
    "knn_con = np.around(knn_con, 2)\n",
    "knn_disp = ConfusionMatrixDisplay(confusion_matrix=knn_con, display_labels= xlabel_General)\n",
    "knn_disp.plot(cmap ='gist_yarg', colorbar=False)\n",
    "plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "plt.savefig(output_dir + 'Nor_KNN_conf.png', dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "#CNN\n",
    "plt.figure(4)\n",
    "cnn_con = confusion_matrix(label, cnn_result, normalize='true')\n",
    "cnn_con_df = pd.DataFrame(cnn_con)\n",
    "cnn_con_df.to_csv(output_dir  + 'Nor_CNN_conf.csv', index= False , header = False)\n",
    "\n",
    "cnn_con = np.around(cnn_con,2)\n",
    "cnn_disp = ConfusionMatrixDisplay(confusion_matrix=cnn_con, display_labels= xlabel_General)\n",
    "cnn_disp.plot(cmap ='gist_yarg',  colorbar=False)\n",
    "plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "plt.savefig(output_dir + 'Nor_CNN_conf.png', dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "## Save ML results to database\n",
    "\n",
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "sql_command = \"\"\"INSERT INTO `Machine_Learning`\n",
    "                        (`method`,`ML_method_id`, `method_parameters_id`,\n",
    "                         `train_set_id`,`train_set_path`, `train_size`,\n",
    "                        `test_set_id` , `test_set_path`, `test_size`,\n",
    "                         `prediction_path`, `accuracy`,\n",
    "                         `confusion_matrix_path`, `confusion_matrix_figure` )\n",
    "               VALUES ('{method}','{ML_method_id}', '{method_parameters_id}',\n",
    "                       '{train_set_id}','{train_set_path}', '{train_size}',\n",
    "                       '{test_set_id}', '{test_set_path}', '{test_size}',\n",
    "                        '{prediction_path}','{accuracy}',\n",
    "                        '{confusion_matrix_path}', '{confusion_matrix_figure}');\"\"\".format(\n",
    "                method = 'RF', ML_method_id = '1', method_parameters_id = '1',\n",
    "                train_set_id = train_set_id, train_set_path = train_set_path, train_size = train_set_size , \n",
    "                test_set_id = test_set_id, test_set_path = test_set_path,  test_size = test_set_size,\n",
    "                prediction_path = output_dir  + 'Nor_RF_prediction.csv', accuracy = rf_acc, \n",
    "                confusion_matrix_path = output_dir + 'Nor_RF_conf.csv', confusion_matrix_figure = output_dir + 'Nor_RF_conf.png')\n",
    "print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n",
    "\n",
    "## Record SVM Results Database\n",
    "sql_command = \"\"\"INSERT INTO `Machine_Learning`\n",
    "                        (`method`,`ML_method_id`, `method_parameters_id`,\n",
    "                         `train_set_id`,`train_set_path`, `train_size`,\n",
    "                        `test_set_id` , `test_set_path`, `test_size`,\n",
    "                         `prediction_path`, `accuracy`,\n",
    "                         `confusion_matrix_path`, `confusion_matrix_figure` )\n",
    "               VALUES ('{method}','{ML_method_id}', '{method_parameters_id}',\n",
    "                       '{train_set_id}','{train_set_path}', '{train_size}',\n",
    "                       '{test_set_id}', '{test_set_path}', '{test_size}',\n",
    "                        '{prediction_path}','{accuracy}',\n",
    "                        '{confusion_matrix_path}', '{confusion_matrix_figure}');\"\"\".format(\n",
    "                method = 'SVM', ML_method_id = '2', method_parameters_id = '1',\n",
    "                train_set_id = train_set_id, train_set_path = train_set_path, train_size = train_set_size , \n",
    "                test_set_id = test_set_id, test_set_path = test_set_path,  test_size = test_set_size,\n",
    "                prediction_path = output_dir  + 'Nor_SVM_prediction.csv', accuracy = svm_acc, \n",
    "                confusion_matrix_path = output_dir + 'Nor_SVM_conf.csv', confusion_matrix_figure = output_dir + 'Nor_SVM_conf.png')\n",
    "print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n",
    "\n",
    "## Record KNN Results Database\n",
    "sql_command = \"\"\"INSERT INTO `Machine_Learning`\n",
    "                        (`method`,`ML_method_id`, `method_parameters_id`,\n",
    "                         `train_set_id`,`train_set_path`, `train_size`,\n",
    "                        `test_set_id` , `test_set_path`, `test_size`,\n",
    "                         `prediction_path`, `accuracy`,\n",
    "                         `confusion_matrix_path`, `confusion_matrix_figure` )\n",
    "               VALUES ('{method}','{ML_method_id}', '{method_parameters_id}',\n",
    "                       '{train_set_id}','{train_set_path}', '{train_size}',\n",
    "                       '{test_set_id}', '{test_set_path}', '{test_size}',\n",
    "                        '{prediction_path}','{accuracy}',\n",
    "                        '{confusion_matrix_path}', '{confusion_matrix_figure}');\"\"\".format(\n",
    "                method = 'KNN', ML_method_id = '3', method_parameters_id = '1',\n",
    "                train_set_id = train_set_id, train_set_path = train_set_path, train_size = train_set_size , \n",
    "                test_set_id = test_set_id, test_set_path = test_set_path,  test_size = test_set_size,\n",
    "                prediction_path = output_dir  + 'Nor_KNN_prediction.csv', accuracy = knn_acc, \n",
    "                confusion_matrix_path = output_dir + 'Nor_KNN_conf.csv', confusion_matrix_figure = output_dir + 'Nor_KNN_conf.png')\n",
    "print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n",
    "\n",
    "## Record CNN Results Database\n",
    "sql_command = \"\"\"INSERT INTO `Machine_Learning`\n",
    "                        (`method`,`ML_method_id`, `method_parameters_id`,\n",
    "                         `train_set_id`,`train_set_path`, `train_size`,\n",
    "                        `test_set_id` , `test_set_path`, `test_size`,\n",
    "                         `prediction_path`, `accuracy`,\n",
    "                         `confusion_matrix_path`, `confusion_matrix_figure` )\n",
    "               VALUES ('{method}','{ML_method_id}', '{method_parameters_id}',\n",
    "                       '{train_set_id}','{train_set_path}', '{train_size}',\n",
    "                       '{test_set_id}', '{test_set_path}', '{test_size}',\n",
    "                        '{prediction_path}','{accuracy}',\n",
    "                        '{confusion_matrix_path}', '{confusion_matrix_figure}');\"\"\".format(\n",
    "                method = 'CNN', ML_method_id = '4', method_parameters_id = '1',\n",
    "                train_set_id = train_set_id, train_set_path = train_set_path, train_size = train_set_size , \n",
    "                test_set_id = test_set_id, test_set_path = test_set_path,  test_size = test_set_size,\n",
    "                prediction_path = output_dir  + 'Nor_CNN_prediction.csv', accuracy = cnn_acc, \n",
    "                confusion_matrix_path = output_dir + 'Nor_CNN_conf.csv', confusion_matrix_figure = output_dir + 'Nor_CNN_conf.png')\n",
    "print (sql_command)\n",
    "cursor.execute(sql_command)\n",
    "mydb.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training size difference evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "training_sizes = [50,100,200,400,800,1600,3200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_calculation(predict, label):\n",
    "    correct = (predict == label).sum().item()\n",
    "    total = label.size(0) \n",
    "    return round(100 * correct / total, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF: Training size difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(train_set_path,header = 0)  ## input_dir + 'SERS_Nor_training.csv'\n",
    "df2 = pd.read_csv(test_set_path, header = 0) ## input_dir + 'SERS_Nor_testing.csv'\n",
    "\n",
    "\n",
    "rf_record = []\n",
    "\n",
    "for training_size in training_sizes:\n",
    "    print('training size is:' + str(training_size))\n",
    "\n",
    "    ## Training\n",
    "    train_feature = df.iloc[:training_size, 1:].to_numpy()\n",
    "    train_label = df.iloc[:training_size,0].to_numpy()\n",
    "\n",
    "    ## Model Training\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(max_depth=40, max_samples=1.0, min_samples_split=5,random_state=0)\n",
    "    rf.fit(train_feature, train_label)\n",
    "\n",
    "    feature = df2.iloc[:, 1:].to_numpy()\n",
    "    label = df2.iloc[:,0].to_numpy()\n",
    "    label = torch.from_numpy(label)\n",
    "\n",
    "    #Random Forest\n",
    "    rf_result = rf.predict(feature)\n",
    "    rf_result_df = pd.DataFrame(rf_result)\n",
    "    rf_result_df.to_csv(output_dir  + 'Nor_RF_%(number)04d_prediction.csv'%{'number': training_size}, index= True , header = False)\n",
    "\n",
    "    rf_result = torch.from_numpy(rf_result)\n",
    "\n",
    "\n",
    "    ## RF_acc\n",
    "    rf_acc = acc_calculation(rf_result, label)\n",
    "    print(f'Random Forest ACC: {rf_acc}%')\n",
    "    rf_record.append(['Random Forest', training_size , rf_acc])\n",
    "\n",
    "\n",
    "    plt.rcParams['font.size'] = 8\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "    #RF\n",
    "    plt.figure()\n",
    "    rf_con = confusion_matrix(label, rf_result,normalize='true')\n",
    "\n",
    "    ## RF confusion matrix dataset csv\n",
    "    rf_con_df = pd.DataFrame(rf_con)\n",
    "    rf_con_df.to_csv(output_dir  + 'Nor_RF_%(number)04d_conf.csv'%{'number': training_size}, index= False , header = False)\n",
    "\n",
    "\n",
    "    rf_con = np.around(rf_con, 2)\n",
    "    rf_disp = ConfusionMatrixDisplay(confusion_matrix=rf_con, display_labels= xlabel_General)\n",
    "    rf_disp.plot(cmap ='gist_yarg', colorbar=False)\n",
    "    plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "    plt.savefig(input_dir + 'Nor_RF_%(number)04d_conf.png' %{'number': training_size}, dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sql_command = \"\"\"INSERT INTO `Machine_Learning`\n",
    "                            (`method`,`ML_method_id`, `method_parameters_id`,\n",
    "                            `train_set_id`,`train_set_path`, `train_size`,\n",
    "                            `test_set_id` , `test_set_path`, `test_size`,\n",
    "                            `prediction_path`, `accuracy`,\n",
    "                            `confusion_matrix_path`, `confusion_matrix_figure` )\n",
    "                VALUES ('{method}','{ML_method_id}', '{method_parameters_id}',\n",
    "                        '{train_set_id}','{train_set_path}', '{train_size}',\n",
    "                        '{test_set_id}', '{test_set_path}', '{test_size}',\n",
    "                            '{prediction_path}','{accuracy}',\n",
    "                            '{confusion_matrix_path}', '{confusion_matrix_figure}');\"\"\".format(\n",
    "                    method = 'RF', ML_method_id = '1', method_parameters_id = '1',\n",
    "                    train_set_id = train_set_id, train_set_path = train_set_path, train_size = training_size , \n",
    "                    test_set_id = test_set_id, test_set_path = test_set_path,  test_size = test_set_size,\n",
    "                    prediction_path = output_dir  + 'Nor_RF_%(number)04d_prediction.csv'%{'number': training_size}, \n",
    "                    accuracy = rf_acc, \n",
    "                    confusion_matrix_path = output_dir + 'Nor_RF_%(number)04d_conf.csv'%{'number': training_size}, \n",
    "                    confusion_matrix_figure = output_dir + 'Nor_RF_%(number)04d_conf.png')\n",
    "    # print (sql_command)\n",
    "    cursor.execute(sql_command)\n",
    "    mydb.commit()\n",
    "\n",
    "rf_record_df = pd.DataFrame(rf_record, columns=['method', 'training_size', 'accuracy'])\n",
    "rf_record_df.to_csv(output_dir + 'Nor_RF_ChangeSize_accuracy.csv', index= False , header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM: Training size difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(train_set_path,header = 0)  ## input_dir + 'SERS_Nor_training.csv'\n",
    "df2 = pd.read_csv(test_set_path, header = 0) ## input_dir + 'SERS_Nor_testing.csv'\n",
    "\n",
    "\n",
    "svm_record = []\n",
    "\n",
    "for training_size in training_sizes:\n",
    "    print('training size is:' + str(training_size))\n",
    "\n",
    "    ## Training\n",
    "    train_feature = df.iloc[:training_size, 1:].to_numpy()\n",
    "    train_label = df.iloc[:training_size,0].to_numpy()\n",
    "\n",
    "    ## Model Training\n",
    "    ## SVM\n",
    "    svm = SVC(C=10)\n",
    "    svm.fit(train_feature, train_label)\n",
    "\n",
    "    feature = df2.iloc[:, 1:].to_numpy()\n",
    "    label = df2.iloc[:,0].to_numpy()\n",
    "    label = torch.from_numpy(label)\n",
    "\n",
    "    ## SVM\n",
    "    svm_result = svm.predict(feature)\n",
    "    svm_result_df = pd.DataFrame(svm_result)\n",
    "    svm_result_df.to_csv(output_dir  + 'Nor_SVM_%(number)04d_prediction.csv'%{'number': training_size}, index= True , header = False)\n",
    "    svm_result = torch.from_numpy(svm_result)\n",
    "\n",
    "\n",
    "    svm_acc = acc_calculation(svm_result, label)\n",
    "    print(f'SVM ACC: {svm_acc}%')\n",
    "    svm_record.append(['SVM', training_size , svm_acc])\n",
    "\n",
    "    plt.rcParams['font.size'] = 8\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## SVM Confusion Matrix\n",
    "    plt.figure()\n",
    "    svm_con = confusion_matrix(label, svm_result,normalize='true')\n",
    "    ## SVM confusion matrix dataset csv\n",
    "    svm_con_df = pd.DataFrame(svm_con)\n",
    "    svm_con_df.to_csv(output_dir  + 'Nor_SVM_%(number)04d_conf.csv'%{'number': training_size}, index= False , header = False)\n",
    "\n",
    "    svm_con = np.around(svm_con, 2)\n",
    "    svm_disp = ConfusionMatrixDisplay(confusion_matrix=svm_con, display_labels= xlabel_General)\n",
    "    svm_disp.plot(cmap ='gist_yarg', colorbar=False)\n",
    "    plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "    plt.savefig(input_dir + 'Nor_SVM_%(number)04d_conf.png' %{'number': training_size}, dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "    sql_command = \"\"\"INSERT INTO `Machine_Learning`\n",
    "                            (`method`,`ML_method_id`, `method_parameters_id`,\n",
    "                            `train_set_id`,`train_set_path`, `train_size`,\n",
    "                            `test_set_id` , `test_set_path`, `test_size`,\n",
    "                            `prediction_path`, `accuracy`,\n",
    "                            `confusion_matrix_path`, `confusion_matrix_figure` )\n",
    "                VALUES ('{method}','{ML_method_id}', '{method_parameters_id}',\n",
    "                        '{train_set_id}','{train_set_path}', '{train_size}',\n",
    "                        '{test_set_id}', '{test_set_path}', '{test_size}',\n",
    "                            '{prediction_path}','{accuracy}',\n",
    "                            '{confusion_matrix_path}', '{confusion_matrix_figure}');\"\"\".format(\n",
    "                    method = 'SVM', ML_method_id = '2', method_parameters_id = '1',\n",
    "                    train_set_id = train_set_id, train_set_path = train_set_path, train_size = training_size , \n",
    "                    test_set_id = test_set_id, test_set_path = test_set_path,  test_size = test_set_size,\n",
    "                    prediction_path = output_dir  + 'Nor_SVM_%(number)04d_prediction.csv'%{'number': training_size}, \n",
    "                    accuracy = svm_acc, \n",
    "                    confusion_matrix_path = output_dir + 'Nor_SVM_%(number)04d_conf.csv'%{'number': training_size}, \n",
    "                    confusion_matrix_figure = output_dir + 'Nor_SVM_%(number)04d_conf.png')\n",
    "    # print (sql_command)\n",
    "    cursor.execute(sql_command)\n",
    "    mydb.commit()\n",
    "\n",
    "svm_record_df = pd.DataFrame(svm_record, columns=['method', 'training_size', 'accuracy'])\n",
    "svm_record_df.to_csv(output_dir + 'Nor_SVM_ChangeSize_accuracy.csv', index= False , header = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN : Training size difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(train_set_path,header = 0)  ## input_dir + 'SERS_Nor_training.csv'\n",
    "df2 = pd.read_csv(test_set_path, header = 0) ## input_dir + 'SERS_Nor_testing.csv'\n",
    "\n",
    "knn_record = []\n",
    "\n",
    "for training_size in training_sizes:\n",
    "    print('training size is:' + str(training_size))\n",
    "\n",
    "    ## Training\n",
    "\n",
    "    train_feature = df.iloc[:training_size, 1:].to_numpy()\n",
    "    train_label = df.iloc[:training_size,0].to_numpy()\n",
    "\n",
    "    ## Model Training\n",
    "    ## KNN \n",
    "    knn = KNeighborsClassifier(algorithm='brute', n_neighbors=10, weights='distance')\n",
    "    knn.fit(train_feature, train_label)\n",
    "\n",
    "\n",
    "    feature = df2.iloc[:, 1:].to_numpy()\n",
    "    label = df2.iloc[:,0].to_numpy()\n",
    "    label = torch.from_numpy(label)\n",
    "\n",
    "    ## KNN Result\n",
    "    knn_result = knn.predict(feature)\n",
    "    knn_result_df = pd.DataFrame(knn_result)\n",
    "    knn_result_df.to_csv(output_dir  + 'Nor_KNN_%(number)04d_prediction.csv'%{'number': training_size}, index= True , header = False)\n",
    "    knn_result = torch.from_numpy(knn_result)\n",
    "\n",
    "\n",
    "    ## KNN Accuracy\n",
    "    knn_acc = acc_calculation(knn_result, label)\n",
    "    print(f'KNN ACC: {knn_acc}%')\n",
    "    knn_record.append(['KNN', training_size , knn_acc])\n",
    "    \n",
    "\n",
    "\n",
    "    plt.rcParams['font.size'] = 8\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "\n",
    "\n",
    "    ## KNN Confusion Matrix\n",
    "    plt.figure()\n",
    "    knn_con = confusion_matrix(label, knn_result,normalize='true')\n",
    "\n",
    "    knn_con_df = pd.DataFrame(knn_con)\n",
    "    knn_con_df.to_csv(output_dir  + 'Nor_KNN_%(number)04d_conf.csv'%{'number': training_size}, index= False , header = False)\n",
    "\n",
    "    knn_con = np.around(knn_con, 2)\n",
    "    knn_disp = ConfusionMatrixDisplay(confusion_matrix=knn_con, display_labels= xlabel_General)\n",
    "    knn_disp.plot(cmap ='gist_yarg', colorbar=False)\n",
    "    plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "    plt.savefig(input_dir + 'Nor_KNN_%(number)04d_conf.png' %{'number': training_size}, dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "    sql_command = \"\"\"INSERT INTO `Machine_Learning`\n",
    "                            (`method`,`ML_method_id`, `method_parameters_id`,\n",
    "                            `train_set_id`,`train_set_path`, `train_size`,\n",
    "                            `test_set_id` , `test_set_path`, `test_size`,\n",
    "                            `prediction_path`, `accuracy`,\n",
    "                            `confusion_matrix_path`, `confusion_matrix_figure` )\n",
    "                VALUES ('{method}','{ML_method_id}', '{method_parameters_id}',\n",
    "                        '{train_set_id}','{train_set_path}', '{train_size}',\n",
    "                        '{test_set_id}', '{test_set_path}', '{test_size}',\n",
    "                            '{prediction_path}','{accuracy}',\n",
    "                            '{confusion_matrix_path}', '{confusion_matrix_figure}');\"\"\".format(\n",
    "                    method = 'KNN', ML_method_id = '3', method_parameters_id = '1',\n",
    "                    train_set_id = train_set_id, train_set_path = train_set_path, train_size = training_size , \n",
    "                    test_set_id = test_set_id, test_set_path = test_set_path,  test_size = test_set_size,\n",
    "                    prediction_path = output_dir  + 'Nor_KNN_%(number)04d_prediction.csv'%{'number': training_size}, \n",
    "                    accuracy = knn_acc, \n",
    "                    confusion_matrix_path = output_dir + 'Nor_KNN_%(number)04d_conf.csv'%{'number': training_size}, \n",
    "                    confusion_matrix_figure = output_dir + 'Nor_KNN_%(number)04d_conf.png')\n",
    "    # print (sql_command)\n",
    "    cursor.execute(sql_command)\n",
    "    mydb.commit()\n",
    "\n",
    "knn_record_df = pd.DataFrame(knn_record, columns=['method', 'training_size', 'accuracy'])\n",
    "knn_record_df.to_csv(output_dir + 'Nor_KNN_ChangeSize_accuracy.csv', index= False , header = True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN: Training size difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure database connection\n",
    "cursor.close()\n",
    "mydb.reconnect()\n",
    "cursor = mydb.cursor(buffered=True)\n",
    "cursor.execute(\"USE `SERS_ML_TEST`;\")\n",
    "\n",
    "\n",
    "\n",
    "cnn_record = []\n",
    "\n",
    "for training_size in training_sizes:\n",
    "    print('training size is:' + str(training_size))\n",
    "\n",
    "\n",
    "    Epoch = 200\n",
    "    BATCH_SIZE = 200\n",
    "    learning_rate = 0.0001 \n",
    "    wd=0.00001\n",
    "\n",
    "\n",
    "    df = pd.read_csv(train_set_path , header = 0)  ## input_dir + 'SERS_Nor_training.csv'\n",
    "    # combine = df.loc[:, 400.0:'abel'].to_numpy()\n",
    "    combine = df.to_numpy()\n",
    "    np.random.seed(8787)\n",
    "    np.random.shuffle(combine)\n",
    "    # feature = combine[:, 1:]\n",
    "    # label =  combine[:, 0]\n",
    "    feature_train = combine[:training_size, 1:]\n",
    "    label_train =  combine[:training_size, 0]\n",
    "    train_size = int(feature_train.shape[0] * 1)\n",
    "\n",
    "\n",
    "    df_test = pd.read_csv(test_set_path, header = 0) ## input_dir + 'SERS_Nor_testing.csv'\n",
    "    # combine = df_test.loc[:, 400.0:'Label'].to_numpy()\n",
    "    combine = df_test.to_numpy()\n",
    "    np.random.seed(8787)\n",
    "    np.random.shuffle(combine)\n",
    "    feature_test = combine[:, 1:]\n",
    "    label_test =  combine[:, 0]\n",
    "    test_size = int(feature_test.shape[0] * 1)\n",
    "    # print(test_size)\n",
    "\n",
    "    ## For conf\n",
    "    label = label_test\n",
    "    label = torch.from_numpy(label)\n",
    "    ###\n",
    "\n",
    "    #Model\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv1d(1, 6, 3, 2)\n",
    "            self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "            self.bn1 = nn.BatchNorm1d(6)\n",
    "            self.conv2 = nn.Conv1d(6, 16, 3, 2)\n",
    "            self.bn2 = nn.BatchNorm1d(16)\n",
    "            self.fc1 = nn.Linear(2288, 280)\n",
    "            self.fc2 = nn.Linear(280, 14)\n",
    "            self.fc3 = nn.Linear(14, 5)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.reshape((x.shape[0],1,-1))\n",
    "            x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "            x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "            x = F.relu(x)\n",
    "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    feature_train = torch.from_numpy(feature_train)\n",
    "    label_train = torch.from_numpy(label_train)\n",
    "    feature_test = torch.from_numpy(feature_test)\n",
    "    label_test = torch.from_numpy(label_test)\n",
    "\n",
    "    train_dataset = Data.TensorDataset(feature_train, label_train)\n",
    "    test_dataset = Data.TensorDataset(feature_test, label_test)\n",
    "    train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "    test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "    #Model Setting\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # if torch.cuda.is_available():\n",
    "    #     print(\"GPU run\")\n",
    "    cnn_model = Net().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(cnn_model.parameters(), lr=learning_rate, weight_decay=wd)\n",
    "\n",
    "\n",
    "    #Model Training\n",
    "    cnn_model_path = input_dir + 'SERS_Nor_CNN_%(number)04d.pth' %{'number': training_size}\n",
    "\n",
    "\n",
    "    accuracy_record = {'train': [], 'test': []} \n",
    "    loss_record = {'train': [], 'test': []} \n",
    "    best_train_acc = 0.0\n",
    "    best_train_loss = 0.0\n",
    "\n",
    "\n",
    "    initial_time = time()\n",
    "\n",
    "    for epoch in range(Epoch):  # loop over the dataset multiple times\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        test_acc = 0.0\n",
    "        test_loss = 0.0\n",
    "\n",
    "        cnn_model.train()\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.type(torch.FloatTensor)\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = cnn_model(inputs)\n",
    "            loss =  criterion(outputs, labels)\n",
    "            _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "        accuracy_record['train'].append(train_acc/len(train_dataset))\n",
    "        loss_record['train'].append(train_loss/len(train_loader))    \n",
    "\n",
    "\n",
    "        cnn_model.eval() # set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.type(torch.FloatTensor)\n",
    "                labels = labels.type(torch.LongTensor)\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = cnn_model(inputs)\n",
    "                loss =  criterion(outputs, labels)\n",
    "                _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                test_acc += (test_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "                test_loss += loss.item()\n",
    "\n",
    "            accuracy_record['test'].append(test_acc/len(test_dataset))\n",
    "            loss_record['test'].append(test_loss/len(test_loader))\n",
    "\n",
    "\n",
    "\n",
    "    torch.save(cnn_model.state_dict(), cnn_model_path)\n",
    "\n",
    "    print('Finished Training (02)')\n",
    "    print('Training time', time() - initial_time)\n",
    "\n",
    "\n",
    "    acc_pd = pd.DataFrame.from_dict(accuracy_record)\n",
    "    loss_pd = pd.DataFrame.from_dict(loss_record)\n",
    "    lc_pd = df = pd.concat([acc_pd,loss_pd], axis=1)\n",
    "    lc_filename =  input_dir + 'SERS_Nor_CNN_%(number)04d_learnCurve.csv'%{'number': training_size}\n",
    "    lc_pd.to_csv(lc_filename, index=True)\n",
    "\n",
    "    #######################################################################\n",
    "    ## Prediction\n",
    "    batch_num = feature_test.shape[0]\n",
    "    test_dataset = Data.TensorDataset(feature_test, label_test)\n",
    "    test_loader = Data.DataLoader(test_dataset, batch_size=batch_num)\n",
    "\n",
    "    cnn_model.eval()\n",
    "    # pred_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data_test in test_loader:\n",
    "            features, labels = data_test\n",
    "            features = features.type(torch.FloatTensor)\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            features = features.reshape((features.shape[0],1, 1, -1))\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = cnn_model(features)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            cnn_result = predicted.to('cpu')\n",
    "\n",
    "    cnn_result = cnn_result.cpu().numpy()\n",
    "    cnn_result_df = pd.DataFrame(cnn_result)\n",
    "    cnn_result_df.to_csv(output_dir  + 'Nor_CNN_%(number)04d_prediction.csv'%{'number': training_size}, index= True , header = False)\n",
    "    \n",
    "    cnn_result = torch.from_numpy(cnn_result)\n",
    "\n",
    "    ## CNN accuracy\n",
    "    cnn_acc = acc_calculation(cnn_result, label)\n",
    "    print(f'CNN ACC: {cnn_acc}%')\n",
    "    cnn_record.append(['CNN', training_size , cnn_acc])\n",
    "    \n",
    "\n",
    "    plt.rcParams['font.size'] = 8\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "\n",
    "    ## CNN Confusion Matrix\n",
    "    plt.figure()\n",
    "    cnn_con = confusion_matrix(label, cnn_result,normalize='true')\\\n",
    "    \n",
    "    cnn_con_df = pd.DataFrame(cnn_con)\n",
    "    cnn_con_df.to_csv(output_dir  + 'Nor_CNN_%(number)04d_conf.csv'%{'number': training_size}, index= False , header = False)\n",
    "    \n",
    "    cnn_con = np.around(cnn_con, 2)\n",
    "    cnn_disp = ConfusionMatrixDisplay(confusion_matrix=cnn_con, display_labels= xlabel_General)\n",
    "    cnn_disp.plot(cmap ='gist_yarg', colorbar=False)\n",
    "    plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "    plt.savefig(input_dir + 'Nor_CNN_%(number)04d_conf.png' %{'number': training_size}, dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "    sql_command = \"\"\"INSERT INTO `Machine_Learning`\n",
    "                            (`method`,`ML_method_id`, `method_parameters_id`,\n",
    "                            `train_set_id`,`train_set_path`, `train_size`,\n",
    "                            `test_set_id` , `test_set_path`, `test_size`,\n",
    "                            `prediction_path`, `accuracy`,\n",
    "                            `confusion_matrix_path`, `confusion_matrix_figure` )\n",
    "                VALUES ('{method}','{ML_method_id}', '{method_parameters_id}',\n",
    "                        '{train_set_id}','{train_set_path}', '{train_size}',\n",
    "                        '{test_set_id}', '{test_set_path}', '{test_size}',\n",
    "                            '{prediction_path}','{accuracy}',\n",
    "                            '{confusion_matrix_path}', '{confusion_matrix_figure}');\"\"\".format(\n",
    "                    method = 'CNN', ML_method_id = '4', method_parameters_id = '1',\n",
    "                    train_set_id = train_set_id, train_set_path = train_set_path, train_size = training_size , \n",
    "                    test_set_id = test_set_id, test_set_path = test_set_path,  test_size = test_set_size,\n",
    "                    prediction_path = output_dir  + 'Nor_CNN_%(number)04d_prediction.csv'%{'number': training_size}, \n",
    "                    accuracy = cnn_acc, \n",
    "                    confusion_matrix_path = output_dir + 'Nor_CNN_%(number)04d_conf.csv'%{'number': training_size}, \n",
    "                    confusion_matrix_figure = output_dir + 'Nor_CNN_%(number)04d_conf.png')\n",
    "    # print (sql_command)\n",
    "    cursor.execute(sql_command)\n",
    "    mydb.commit()\n",
    "\n",
    "cnn_record_df = pd.DataFrame(cnn_record, columns=['method', 'training_size', 'accuracy'])\n",
    "cnn_record_df.to_csv(output_dir + 'Nor_CNN_ChangeSize_accuracy.csv', index= False , header = True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "899abd191a9ea73a89afb19849dc406f29b830785484ec2528e59ce4ca659ec1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
